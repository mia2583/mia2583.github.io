---
title: "[AI] 인공지능 소개"
date: 2025-01-02 22:00:00 +09:00
categories: 인공지능
description: 데이터 분석과 인공지능에 대해 소개한다.
pin: true
use_math: true
---

이번 포스팅은 데이터 분석과 인공지능의 기술 발전에 따른 논의점과 주의 사항에 대해 간단히 알아본다.

## 1. 인공지능 학습 시 주의사항
### 1-1. 데이터 전처리
학습에 사용할 데이터의 품질이 인공지능 모델의 품질로 연결되기 때문에 데이터의 전처리 과정은 매우 중요하다. 

특히 데이터를 해석할 때, 상관 관계와 인과 관계를 구분하여 사용해야 한다.

<img src="{{ site.baseurl }}/assets/img/post/AI/chocolate_nobel_prize.JPG" alt="초콜렛과 노벨 수상자 수" style="width: 50%">

위의 이미지는 초콜렛 소비량과 노벨상 수상자 수와의 상관관계를 조사한 통계이다. 하지만 이 통계를 바탕으로, 초콜릿 소비량과 노벨상 수상자 수 사이를 직접적인 영향이 있는 인관 관계(한 사건이 다른 사건의 원인이 됨)로 해석하는 것은 잘못된 것이다. 이 경우에는 초콜릿 소비량과 노벨상 수상자 수 사이에 단순히 우연적인 상관 관계가 있다고 해석하는 것이 옳다. 

데이터를 에러바를 사용해 시각화하거나, 아웃라이어를 제거, 데이터 표준화 등을 통해 데이터 자체를 충분히 살펴보는 시간이 필요하다.

### 1-2. 데이터의 양
인공지능 모델이 일반적으로 100만 정도의 데이터가 있어야 많은 수의 파라미터를 학습 할 수 있다고 한다.

<img src="{{ site.baseurl }}/assets/img/post/AI/model_fitting.JPG" alt="모델 학습 예시" style="width: 60%">

모델이 너무 단순해서 충분히 학습하지 못한 것을 '언더 피팅'이라 하며, 위의 그림에서 왼쪽과 같이 O와 X를 제대로 분류하지 못한다. 반대로 과도하게 학습하는 것을 '오버 피팅'이 하는데, 위 이미지의 오른쪽과 같이 O와 x을 민감하게 분리한다. 이런 경우에는 데이터가 조금만 달라져도 좋지 않은 결과가 반환될 수 있다. 

따라서 중간 이미지와 같이 어느 정도 유연성 있는 모델이 만들어지도록 적절히 학습시켜야 한다. 또한 모델의 성능을 객관적으로 평가하기 위해 학습 데이터와 테스트 데이터를 분리할 필요가 있다.

## 2. 블랙박스 알고리즘
인공지능 모델은 학습된 파라미터를 바탕으로 계산된 결과를 출력한다. 하지만 입력된 데이터가 어떻게 조합되고 변형 되었는지, 그리고 어떤 특징이 최종 결과에 어떻게 기여가 되었는지 추적하기 어렵다. 이처럼 사람의 시각에서 모델이 "왜" 이러한 결정을 내렸는지 추론하기 쉽지 않기 때문에 인공지능 모델을 블랙박스 알고리즘이라고 표현한다. 

하지만 모델이 실생활에 적용 되기 위해서는 성능뿐 만이 아니라 모델의 설명력이 필요하다. 이를 위해 모델 알고리즘의 내면을 가시화해서 보여주는 것을 사후 설명력(post-hoc explanability)이라 하며, saliency map, SHAP와 같은 기술이 개발되고 있다.

아래 이미지는 saliency map의 한 예시이다. 

<img src="{{ site.baseurl }}/assets/img/post/AI/saliency_map_ex.JPG" alt="saliency map 예시" style="width: 70%">

왼쪽 이미지는 모델이 고양이와 관련된 영역을 하이라이트하여 고양이라고 답한 이유를 설명하고 있다. 오른쪽 이미지는 모델이 강아지와 관련된 영역을 하이라이트하고 있다.

### 2-1. One Pixel Attack
사후 설명력이 모델에 대해 항상 신뢰성을 주는 것은 아니다. 대표적으로 One Pixel Attack 공격을 예로 들 수 있다. 

<img src="{{ site.baseurl }}/assets/img/post/AI/one_pixel_attack_ex.JPG" alt="one pixel attack의 예시" style="width: 70%">

One Pixel Attack은 이미지의 하나의 픽셀만을 바꿨을 뿐인데 잘못된 예측을 반환한다. 이는 블랙박스 알고리즘 모델에는 일부 특징에 민감하게 반응하는 등 예상치 못한 취약점이 있을 수 있음을 알려준다.

## 3. 인공지능의 신뢰도

### 3-1. 웹 데이터 사용 시 주의사항
웹 데이터를 사용할 때에는 해당 데이터의 대표성, 진실성에 대해 주의해야 한다. Spiral of silence와 같은 현상에 의해서 인터넷 상의 의견이 대표성 있는 의견이 아닐 수 있다. 또한 가짜 정보들이 빠르게 전파되는 요즘에는 데이터의 진실성에 대해서도 살펴볼 필요가 있다.

> Spiral of silence(침묵의 나선 이론)?  
> 사람들이 자신이 소수 의견을 가지고 있다고 느낄 때, 사회적 고립을 피하기 위해서 침묵을 지키는 현상  

> Infodemic(임포데믹)?  
> 사실 정보와 더불어 오정보의 양이 늘어 구분이 어려워지는 정보 과부하 현상

### 3-2. 알고리즘의 편향성
인공지능 알고리즘은 블랙박스이기에 예상치 못한 부작용이 있을 수 있다. 아래는 편향적으로 학습된 모델의 예시들이다.

<img src="{{ site.baseurl }}/assets/img/post/AI/compas_prediction.JPG" alt="COMPAS의 재범률 예측" style="width: 80%">

피고의 재범률을 평가하는 시스템인 COMPAS 제도는 일부 연구에서 인종적 편향을 내포하여 특정 인종에게 불리한 결과를 낳을 수 있다는 지적이 있다.

<img src="{{ site.baseurl }}/assets/img/post/AI/tay.JPG" alt="챗봇 Tay" style="width: 60%">

마이크로소프트가 출시한 챗봇 Tay는 몇몇의 사용자의 악의적인 조작으로 인해 혐오 발언, 인종 차별적, 성적 발언을 하도록 학습이 되어 출시된지 16시간 만에 서비스가 중단되었다. 

이후 민감한 주제에 대해 대답을 회피하는 새로운 챗봇, Zo을 출시하였지만, 자유로운 의견 교환을 할 수 없어 검열된 정보만을 학습한다는 비판을 받았다.

데이터가 편향되면 알고리즘 결과 또한 편향될 수 밖에 없다. 알고리즘이 데이터에 들어있는 편향을 반영하는 것을 피할 순 없지만, 역으로 우리가 만든 알고리즘이 사회의 편향을 조장하는 것은 아닌지 항상 유의해야 한다.

## 4. 인공지능의 발전
인공지능은 놀라운 속도로 빠르게 발전하고 있다. GAN(적대적 생성 신경망)을 통해서 자연어 처리 분야에서 질문 응답 모델을 평가하기 위해 사용되는 SQuAD 2.0에서는 인공지능의 문맥 이해 능력이 이미 사람의 성능을 초월했음을 보여준다.
또한 인공지능 모델이 창작한 소설이 일본의 호시 신이치 문학상 SF 부문에서 1차 예선을 통과하기도 하고, 또 다른 모델은 교향곡을 작곡해 연주되기도 했다.

이처럼 빠르게 발전하고 있는 새로운 기술들 속에서 한편으로는 법과 윤리의 부재가 대두되고 있다.

### 4-1. 저작권
인공지능의 저작권 문제는 많은 분야에서 논의되어 왔다. 하지만 데이터 제공자, 모델 개발자, 작품 개발자 등 저작권에 대한 분류 기준이 애매하고 혜택 제공 방안도 마련하기 어렵다. 법인 등록과 같이 AI에게 법적 권리를 부여하는 등에 대해 논의 중이지만 여전히 찬반 논란이 많다. 

### 4-2. 윤리적 문제
인공지능과 관련되어 논의되어야 할 윤리적 문제도 많다. 우버회사의 자율주행 자동차가 보행자를 죽인 사고가 발생한 적이 있다. 하지만 해당 문제에 대해 법적 책임자와 처벌에 대한 논의는 아직까지 마무리되지 못했다.

## 참고
본 포스팅은 LG Aimers 강좌 중 KAIST 차미영 교수님의 'AI 윤리'에서 학습한 내용을 정리한다.



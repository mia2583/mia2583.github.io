---
title: "[AI] 인공지능 소개"
date: 2025-01-02 22:00:00 +09:00
categories: 인공지능
description: 데이터 분석과 인공지능에 대해 소개한다.
pin: true
use_math: true
---

이번 포스팅은 데이터 분석과 인공지능의 기술 발전에 따른 논의점과 주의 사항에 대해 간단히 알아본다.

## 1. 인공지능 학습 시 주의사항
### 1-1. 데이터 전처리
학습에 사용할 데이터의 품질이 인공지능 모델의 품질로 연결되기 때문에 데이터의 전처리 과정은 매우 중요하다. 특히 데이터를 해석할 때, 상관 관계와 인과 관계를 명확히 구분하여 사용해야 한다.

<img src="{{ site.baseurl }}/assets/img/post/AI/chocolate_nobel_prize.JPG" alt="초콜렛과 노벨 수상자 수" style="width: 70%">

위의 이미지는 초콜렛 소비량과 노벨상 수상자 수와의 상관관계를 조사한 통계이다.
하지만 이 두 사건을 직접적인 영향이 있는 인관 관계(한 사건이 다른 사건의 원인이 됨)로 해석하는 것은 잘못된 것이다. 이 경우에는 초콜릿 소비량과 노벨상 수상자 수 사이에 단순히 우연적인 상관 관계가 있다고 해석하는 것이 옳다. 

데이터를 에러바를 사용해 시각화하거나, 아웃라이어를 제거, 데이터 표준화 등을 통해 데이터 자체를 충분히 살펴보는 시간이 필요하다.

### 1-2. 데이터의 양
인공지능 모델이 일반적으로 100만 데이터가 있어야 많은 수의 파라미터를 학습 할 수 있다고 한다.

<img src="{{ site.baseurl }}/assets/img/post/AI/model_fitting.JPG" alt="모델 학습 예시" style="width: 70%">

모델이 너무 단순해서 충분히 학습하지 못한 것을 '언더 피팅'이라 하며, 위의 그림에서 왼쪽과 같이 O와 X를 제대로 분류하지 못한 상황을 말한다.

반대로 과도하게 학습하는 것을 '오버 피팅'이 하는데, 위 이미지의 오른쪽은 O와 x을 민감하게 분리한다. 이런 경우에는 데이터가 조금만 달라져도 좋지 않은 결과가 반환될 수 있다. 

따라서 우리는 중간 이미지와 같이 어느 정도 유연성 있는 모델이 만들어지도록 적절히 학습해야 한다.

또한 모델의 성능을 객관적으로 평가하기 위해 학습 데이터와 테스트 데이터를 분리할 필요가 있다.

## 2. 블랙박스 알고리즘
인공지능 모델은 학습된 파라미터를 바탕으로 계산된 결과를 출력한다. 하지만 입력된 데이터가 어떻게 조합되고 변형 되었는지, 그리고 어떤 특징이 최종 결과에 어떻게 기여가 되었는지 추적하기 어렵다. 이처럼 사람의 시각에서 모델이 "왜" 이러한 결정을 내렸는지 추론하기 쉽지 않기 떄문에 인공지능 모델을 블랙박스 알고리즘이라고 표현한다. 

하지만 실생활에 적용이 되기 위해서는 성능뿐 만이 아니라 모델의 설명력이 필요하다. 이를 위해 알고리즘의 내면을 가시화해서 보여주는 것을 사후 설명력(post-hoc explanability)이라 하며, saliency map, SHAP와 같은 기술이 개발되고 있다.

아래 이미지는 saliency map의 한 예시이다. 

<img src="{{ site.baseurl }}/assets/img/post/AI/saliency_map_ex.JPG" alt="saliency map 예시" style="width: 70%">

왼쪽 이미지는 모델이 고양이와 관련된 영역을 하이라이트하여 고양이라고 답한 이유를 설명하고 있다. 오른쪽 이미지는 모델이 강아지와 관련된 영역을 하이라이트하고 있다.

### 2-1. One Pixel Attack
사후 설명력이 모델에 대해 항상 신뢰성을 주는 것은 아니다. 대표적으로 One Pixel Attack 공격을 예로 들 수 있다. 

<img src="{{ site.baseurl }}/assets/img/post/AI/one_pixel_attac_ex.JPG" alt="one pixel attack의 예시" style="width: 70%">

One Pixel Attack은 이미지의 하나의 픽셀만을 바꿨을 뿐인데 잘못된 예측을 반환한다. 이는 블랙박스 알고리즘 모델에는 일부 특징에 민감하게 반응하는 등 예상치 못한 취약점이 있을 수 있음을 알려준다.

## 3. 인공지능의 신뢰도

### 3-1. 웹 데이터 사용 시 주의사항
웹 데이터를 사용할 때에는 해당 데이터의 대표성, 진실성에 대해 주의해야 한다. Spiral of silence와 같은 현상에 의해서 인터넷 상의 의견이 대표성 있는 의견이 아닐 수 있다. 또한 가짜 정보들이 빠르게 전파되는 요즘에는 데이터의 진실성에 대해서도 살펴볼 필요가 있다.

> Spiral of silence(침묵의 나선 이론)?  
> 사람들이 자신이 소수 의견을 가지고 있다고 느낄 때, 사회적 고립을 피하기 위해서 침묵을 지키는 현상  
> Infodemic(임포데믹)?  
> 사실 정보와 더불어 오정보의 양이 늘어 구분이 어려워지는 정보 과부하 현상

### 3-2. 편향된 학습
인공지능 알고리즘은 블랙박스이기에 예상치 못한 부작용이 있을 수 있다. 아래는 편향적으로 학습된 모델의 예시들이다.

<img src="{{ site.baseurl }}/assets/img/post/AI/compas_prediction.JPG" alt="COMPAS의 재범률 예측" style="width: 70%">

피고의 재범률을 평가하는 시스템인 COMPAS 제도는 일부 연구에서 인종적 편향을 내포하여 특정 인종에게 불리한 결과를 낳을 수 있다는 지적이 있다.

<img src="{{ site.baseurl }}/assets/img/post/AI/tay.JPG" alt="챗봇 Tay" style="width: 70%">

마이크로소프트가 출시한 챗봇 Tay는 몇몇의 사용자의 악의적인 조작으로 인해 혐오 발언, 인종차별적, 성적 발언을 하도록 학습이 되어 출시된지 16시간 만에 서비스가 중단되었다. 

이후 민감한 주제에 대해 대답을 회피하는 새로운 챗봇, Zo을 출시하였지만, 자유로운 의견 교환을 할 수 없어 검열된 정보만을 학습한다는 비판을 받았다.

## 4. 인공지능의 발전

### 3-2. 저작권

### 3-3. 이종 데이터의 결합


## 참고
본 포스팅은 LG Aimers 강좌 중 KAIST 차미영 교수님의 'AI 윤리'에서 학습한 내용을 정리한다.



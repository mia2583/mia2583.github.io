---
title: "[RL] FrozenLake에서 Q Network 학습하기"
date: 2024-12-28 21:30:00 +09:00
categories: 강화학습
description: Q Network를 사용하여 FrozenLake를 학습해본다.
pin: true
use_math: true
---
Q 테이블을 이용한 학습은 성능이 좋지만, 상태(state) 공간이 커지면 테이블 크기가 커져 실생활 적용엔 무리가 있다. 따라서 이번 포스팅에서는 Neural Network를 이용해 학습을 해보고자 한다.

## 1. Neural Network(인공 신경망)
인공 신경망은 두뇌의 뉴런을 모방한 모델이다. 마치 하나의 뉴런이 다른 여러 뉴런으로부터 입력 신호를 받아 출력 신호를 내보내는 것처럼 인공 신경망은 입력 값을 받아 처리 후 출력 값을 내보낸다. 

// 신경망 이미지

인공 신경망은 여러개의 층(layer)를 쌓아서 만들 수 있는데, 입력을 받는 곳을 입력층(input layer), 데이터 처리 및 학습이 일어나는 곳을 은닉층(hidden layer), 최종 결과를 내는 곳이 출력층(output layer)이다. 하나의 인공 신경망은 여러개의 은닉층을 가질 수 있으며, 은닉층이 많아질수록 학습 능력이 증가하지만, 계산 복잡성과 과적합 위험이 높아질 수 있으므로 적절한 균형이 필요하다.

신경망이 학습을 잘 진행하고 있는지 평가하는 함수를 cost 함수라 하는데, 이는 예측값과 실제값(정답) 사이의 차이를 측정하는 방식으로 정의된다. 아래는 cost 함수 종류 중의 하나인 MSE(Mean Square Error, 평균 제곱 오차)이다.

$$
cost(W) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \quad (n은\;데이터의\;수)
$$

## 2. Q Network
강화학습에서 사용되는 신경망 기반 학습 알고리즘을 Q Network라 한다. FrozenLake에서는 state를 입력으로 넣고 그에 따른 action을 출력하는 신경망을 구성할 수 있다. 

// 그림

입력을 s, 각 레이어들의 가중치의 집합을 W라 하면 출력은 Ws가 된다. 이 Ws가 Q 테이블에서의 Q(s, a)와 같으므로 이 Ws가 최적의 Q 값(y)을 갖는 방향으로 학습을 해야한다. 그러므로 cost 함수는 아래와 같이 정의할 수 있다.

$$
cost(W) = \frac{1}{m} \sum_{i=1}^{m} (Wx^{(i)} - y^{(i)})^2
$$

Q 테이블에서 y의 값을 $r +\gamma\;maxQ(s', a')$로 정의했었다. cost 함수의 평균을 아래와 같이 다시 표현할 수 있다.

$$
\begin{align*}
cost(W) &= \frac{1}{m} \sum_{i=1}^{m} (Wx^{(i)} - y^{(i)})^2 \\
&= \frac{1}{m} \sum_{i=1}^{m} (\hat{Q}(s_i, a_i) - r_i + \gamma\;max_{a'} \hat{Q}(s_{t+1}, a'))^2 \\
\end{align*}
$$

그리고 이 cost 함수를 최소화하는 파라미터들의 집합 $\theta$를 구해야한다.

### 2-1. 가중치(W)와 파라미터($\theta$)의 차이
//

### 2-2. Stochastic?
신경망을 학습시키기 위해서 Q 함수로 $r +\gamma\;maxQ(s', a')$를 사용하였다. 하지만 해당 수식은 stochastic이 아니라 deterministic한 환경에서 사용하던 수식이었다.

왜 deterministic 수식을 사용하는 것일까?
Stochastic한 함수를 새로 정의했던 이유를 다시 생각해보자.

Stochastic 환경에서는 action에 따른 결과가 불안정했다. 그렇기에 Q 함수가 업데이트 될 때 값이 한번에 학습되는 것이 아니라, 원래의 값을 유지하면서 조금씩 학습을 받아들이도록 수식을 변형하였다.

인공 신경망은 cost 함수를 통해 조금씩 학습하므로 식을 변형하지 않아도 동일한 효과를 나타낸다.

## 3. 실습(Q Network)

### 3-1. 실행 결과

### 3-2. 성능이 좋지 않은 이유

## 참고
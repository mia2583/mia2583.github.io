---
title: "[AI] 지도 학습1"
date: 2025-01-08 19:30:00 +09:00
categories: 인공지능
description: 지도학습 중 회귀에 대해 자세히 알아본다.
pin: true
use_math: true
---

## 1. 지도학습

지도 학습은 컴퓨터에게 문제와 정답 쌍을 제공하는 형태로 학습하는 방법을 말한다. 지도 학습의 예로는 아래와 같은 것들이 있다. 

- Image Classification : 이미지의 카테고리를 맞히는 문제
- Text Classification : 주어진 문장이 긍정적인지 부정적인지 파악하는 문제
- Next Word Prediction : 주어진 글에 이어질 단어를 예측하는 문제
- Translation : 주어진 문장을 다른 언어로 번역하는 문제
- Price Prediction : 이전 10일치의 가격을 보고 다음날 주식 가격을 예측하는 문제

지도학습의 종류는 크게 분류와 회귀로 나눠진다. 분류는 고를 수 있는 정답이 정해져있어 주어진 입력에 대해 속하는 정답을 선택하는 문제이고, 회귀는 연속적인 수치 데이터를 예측하는 문제이다. 먼저 회귀 문제에 대해 자세히 알아본다.

### 2. 회귀(Regression)

먼저 모든 데이터를 완벽히 표현하는 정답 함수를 f*이라고 하자. 그러면 우리는 이러한 f*에 최대한 근접한 함수 g를 구하는 것이 목표이다. 이때, 함수 g에 대해 너무 큰 자유도가 주어지면 문제 해결이 어려울 수 있으므로 함수 후보군(=함수 클래스)인 G를 미리 정의한다. 즉, G 함수 중에서 가장 f*에 근접한 함수 g를 찾는 목표로 축소시키는 것이다. 

예를 들어, 주어진 키를 바탕으로 몸무게를 예측하는 모델을 개발하고자 한다. 데이터를 완벽히 표현하는 정답 함수 f*의 형태를 모르지만 일단 우리는 선형 함수 중에서 g를 찾고자 한다. 그러면 함수 클래스 G는 선형 함수로 정의된다. 
- $G = g_{a,b}(x) = g_{\theta}(x) = ax+b $ (x는 입력, $\theta = (a,b)$)

### 2-1. 손실함수

<img src="{{ site.baseurl }}/assets/img/post/AI/height_weight.JPG" alt="키에 따른 몸무게" style="width: 70%">

하지만 위 이미지에서 보다시피 선형 함수로는 모든 데이터를 완벽히 표현할 수 없다. 그러면 선형 함수와 f*의 근사도는 어떻게 측정할 수 있을까?

<img src="{{ site.baseurl }}/assets/img/post/AI/height_weight_loss.JPG" alt="키에 따른 몸무게 손실 계산" style="width: 90%">

하나의 데이터 $x^{(i)}$에 대해서 생각해보자. 함수 f*에 따른 출력은 $y^{(i)}$이어야 한다. 그리고 g함수의 출력은 $g_{\theta}(x^{(i)})$ 이다. 이 둘의 근사도는 오차 계산방법인 MSE(Mean Square Error)로 계산할 수 있다. 

$$
\begin{align*}
loss &= l(g_{\theta}(x^{(i)}), y^{(i)}) \\
&= (g_{\theta}(x^{(i)}) - y^{(i)})^2
\end{align*}
$$

따라서 모든 입력 데이터에 대한 MSE는 다음과 같이 계산된다.

$$
L(\theta) = \sum_{i=1}^n (g_{\theta}(x^{(i)}) - y^{(i)})^2
$$

> MSE는 왜 절댓값이 아닌 제곱을 사용할까?  
> 제곱을 하면 큰 오차에 대해 더 큰 패널티를 부여할 수 있고 미분이 가능하기 때문에 계산에 용이하다.

MSE(회귀 문제에 주로 사용)외에도 cross-entropy(분류 문제에 자주 사용)와 같이 상황에 맞게 다른 오차 계산법을 사용할 수 있다. $L(\theta)$를 최소로 만드는 a, b의 값은 편미분(gradient)를 통해서 구할 수 있다. multidimensio에서도 마찬가지로 편미분으로 구할 수 있다.

> Multi-dimension에서의 손실함수?  
> 데이터를 $x = (x_1, x_2, ..., x_d)$라고 하면 G함수는 $a^Tx+b$ 형태를 가진다.이때 b를 1xb로 해석하면, $x=(1, x_1, x_2, ..., x_d)$이고 G 함수는 $a^TX$로 표현할 수 있다. (이때, $a=(a_0, a_1, ..., a_d)) 그러면 손실 함수는 다음과 같이 정의할 수 있다.  
> 
$$
\begin{align*} 
L(a) &= \sum_{i=1}^n (g_{a}(x^{(i)}) - y^{(i)})^2 \\
&= \left\| \begin{bmatrix} a^Tx^{(1)}-y^{(1)} \\ a^Tx^{(2)}-y^{(2)} \\ ... \\ a^Tx^{(n)}-y^{(n)} \end{bmatrix} \right\|^2 \\
&= || Xa - Y ||^2 \\
&= Y^TY - 2Y^TXa + a^TX^TXa 
\end{align*}
$$  
>  
> 또한 손실 함수를 최소로하는 a의 값에 대한 식은 아래와 같이 정리할 수 있다. 
>   
$$
\frac{\partial L(a)}{\partial a} = -2X^TY + 2X^TXa = 0 \\
\therefore a = (X^TX)^{-1}X^TY 
$$  
>   
> 위의 식을 통해서 다른 문제에서도 a값을 쉽게 구할 수 있다. 

### 2-2. 다차원 모델

만약 선형 함수가 아니라 2차, 3차 함수로 모델을 생성하고 싶다면 어떻게 될까? 예를 들어 $X'=(1, x, x^2)$이라고 하자 그러면 $g_a(x) = a_0+a_1x+a_2x^2 = aX'$과 같이 정의할 수 있을 것이다. 우리가 구하고자 하는 것은 a이므로 x의 차수가 변하더라도 a_1, a_2, ...의 입장에서는 여전히 일차 함수인 셈이다. 따라서 위와 동일한 방식으로 행렬 a의 값을 구할 수 있다. 이처럼 모델의 차수가 커질수록 더 많은 것을 표현할 수 있지만 항상 오버피팅에 주의해야 한다.





